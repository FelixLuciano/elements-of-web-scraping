{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dense-buffer",
   "metadata": {},
   "source": [
    "# Análise de texto de fontes desestruturadas e Web\n",
    "\n",
    "## Exercícios da Aula 09\n",
    "\n",
    "Este notebook servirá para relembrarmos **requests** e **BeautifulSoup**, que serão utilizadas para baixar notícias do site da **IstoÉ Dinheiro**. Em seguida, iremos extrair informações para construir um Pandas DataFrame de títulos e descrições para prática de **RegEx**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-football",
   "metadata": {},
   "source": [
    "## Importando as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "motivated-lender",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O seu notebook está na pasta:\n",
      "/home/jovyan\n"
     ]
    }
   ],
   "source": [
    "# para nos comunicarmos com a Web\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "# para extrair informações de páginas HTML\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Para criar um Data Frame\n",
    "import pandas as pd\n",
    "\n",
    "# Para expressões regulares\n",
    "import re\n",
    "\n",
    "# Para PDFs\n",
    "import PyPDF2 as pp\n",
    "\n",
    "# Recursos do sistema\n",
    "import os\n",
    "\n",
    "print('O seu notebook está na pasta:')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-idaho",
   "metadata": {},
   "source": [
    "## Definindo cabeçalho User-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-macedonia",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-opposition",
   "metadata": {},
   "source": [
    "## Definindo em qual página buscar\n",
    "\n",
    "Vamos definir qual seção iremos utilizar para baixar as notícias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-venture",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "secao = 'economia'\n",
    "url = f'https://www.istoedinheiro.com.br/categoria/{secao}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-setup",
   "metadata": {},
   "source": [
    "## Utilizando *requests* para baixar a página de notícias\n",
    "\n",
    "Com o uso da biblioteca **requests**, podemos obter o **HTML** da página da IstoÉ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-incidence",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "resposta = requests.get(url = url, headers=headers)\n",
    "\n",
    "resposta.encoding = 'utf-8'\n",
    "\n",
    "resposta.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-battle",
   "metadata": {},
   "source": [
    "## Extraindo informações relevantes com *BeautifulSoup*\n",
    "\n",
    "Perceba que o HTML inclui uma grande quantidade de tags, o que dificulta identificar informações relevantes de forma direta. Com o auxílio da biblioteca **BeautifulSoup** podemos extrair facilmente as informações que desejamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-scott",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resposta.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-creator",
   "metadata": {},
   "source": [
    "Agora, vamos obter uma lista com todos os trechos HTML que contém uma notícia. Para identificar as tags corretas, é preciso ir até a página Web que desejamos extrair informações, dar botão direito e ir em **inspecionar elemento**, navegando pelo HTML até identificar as tags necessárias\n",
    "\n",
    "Ex: https://www.istoedinheiro.com.br/categoria/economia/politica/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-recognition",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "lista_tag_noticia = soup.find_all('article', class_='thumb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-lighter",
   "metadata": {},
   "source": [
    "Agora, podemos passar por cada uma das chamadas de notícias, extraindo informações de interesse, como o título, descrição e data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-timeline",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "lista_titulo = []\n",
    "lista_desc = []\n",
    "lista_data = []\n",
    "\n",
    "for i in range(0, len(lista_tag_noticia)):\n",
    "    \n",
    "    tag_noticia = lista_tag_noticia[i]\n",
    "\n",
    "\n",
    "    titulo = tag_noticia.find('h3').text\n",
    "    titulo = titulo.replace('\\n', '') #limpa os ENTERS a esquerda e direita\n",
    "    lista_titulo.append(titulo)\n",
    "\n",
    "    descricao = tag_noticia.find('p').text\n",
    "    lista_desc.append(descricao)\n",
    "    \n",
    "    data_hora = tag_noticia.find('time', class_='c-data').text\n",
    "    lista_data.append(data_hora)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-karma",
   "metadata": {},
   "source": [
    "## Criando um DataFrame\n",
    "\n",
    "As informações que consideramos foram extraídas na repetição **for** e armazenadas em listas. Podemos utilizar estas listas para construir um Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-dependence",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Secao': secao,\n",
    "                   'Titulo': lista_titulo,\n",
    "                   'Descrição': lista_desc,\n",
    "                   'Data': lista_data\n",
    "                  })\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd975df",
   "metadata": {},
   "source": [
    "## Fazer o scraping de várias páginas\n",
    "\n",
    "Ao navegar pelo site, percebemos que a **URL** das páginas segue o seguinte padrão: https://www.istoedinheiro.com.br/categoria/economia/page/2/\n",
    "\n",
    "Vamos construir algumas funções auxiliares e fazer a extração dos dados das notícias de várias páginas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee753ac",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_html_of_page(secao = 'economia', page_number=1):\n",
    "    url = f'https://www.istoedinheiro.com.br/categoria/{secao}/page/{page_number}'\n",
    "    headers = ({'User-Agent':\n",
    "            'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'})\n",
    "    resposta = requests.get(url = url, headers=headers)\n",
    "    resposta.encoding = 'utf-8'\n",
    "    return resposta.text\n",
    "\n",
    "def get_data_of_page(secao = 'economia', page_number=1):\n",
    "    html = get_html_of_page(secao=secao, page_number=page_number) \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    lista_tag_noticia = soup.find_all('article', class_='thumb')\n",
    "\n",
    "    lista_titulo = []\n",
    "    lista_desc = []\n",
    "    lista_data = []\n",
    "\n",
    "    for i in range(0, len(lista_tag_noticia)):\n",
    "\n",
    "        tag_noticia = lista_tag_noticia[i]\n",
    "\n",
    "        titulo = tag_noticia.find('h3').text\n",
    "        titulo = titulo.replace('\\n', '') #limpa os ENTERS a esquerda e direita\n",
    "        lista_titulo.append(titulo)\n",
    "\n",
    "        descricao = tag_noticia.find('p').text\n",
    "        lista_desc.append(descricao)\n",
    "\n",
    "        data_hora = tag_noticia.find('time', class_='c-data').text\n",
    "        lista_data.append(data_hora)\n",
    "\n",
    "    return lista_titulo, lista_desc, lista_data\n",
    "\n",
    "def get_dataframe_of_page(secao = 'economia', page_number=1):\n",
    "    lista_titulo, lista_desc, lista_data = get_data_of_page(secao=secao, page_number=page_number)\n",
    "    df = pd.DataFrame({'Secao': secao,\n",
    "                       'Titulo': lista_titulo,\n",
    "                       'Descrição': lista_desc,\n",
    "                       'Data': lista_data\n",
    "                      })\n",
    "    return df\n",
    "\n",
    "def get_news(secoes, n_pages=5):\n",
    "    dfs = []\n",
    "    for secao in secoes:\n",
    "        for i in range(1, n_pages+1):\n",
    "            dfs.append(get_dataframe_of_page(secao=secao, page_number=i))\n",
    "    return pd.concat(dfs, axis=0)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8dc3bf",
   "metadata": {},
   "source": [
    "Vamos extrair dados de algumas seções, sendo das páginas 1..5 para cada seção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4faf59e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = get_news(['economia', 'negocios', 'mercado-digital'], n_pages=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f316b52",
   "metadata": {},
   "source": [
    "E visualizar algumas notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d595c0c2",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-intermediate",
   "metadata": {},
   "source": [
    "### Salvando o DataFrame em CSV\n",
    "\n",
    "É interessante armazenar o Dataframe em CSV para que ele possa ser análisado em algum momento posterior. Você poderia, por exemplo, extrair as notícias todos os dias de uma semana e analisar somente após ter todos estes dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-certificate",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('noticias_200423.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26a441",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('notebooks/lessons/09/noticias_200423.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e404b5",
   "metadata": {},
   "source": [
    "## Relembrando - Extração de textos de PDFs\n",
    "\n",
    "Vamos relembrar o que vimos na aula de extração de textos de PDFs e juntar com Expressões Regulares para procurarmos por padrões interessantes.\n",
    "\n",
    "Primeiro, vamos fazer o download de uma página qualquer do diário oficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0297ef2e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pdf_url=\"http://diariooficial.imprensaoficial.com.br/doflash/prototipo/2023/Abril/19/exec1/pdf/pg_0001.pdf\"\n",
    "\n",
    "response = urllib.request.urlopen(pdf_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ebdc0e",
   "metadata": {},
   "source": [
    "Vamos salvar a resposta obtida em um arquivo PDF chamado `'pg_0001.pdf'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e53db",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "with open('pg_0001.pdf', 'wb') as arq:\n",
    "    arq.write(response.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cc60d7",
   "metadata": {},
   "source": [
    "Então podemos utilizar a biblioteca `PyPDF2` para extrair os textos. Aqui, pense que nosso objetivo será apenas identificar **CPF**, **CNPJ**, datas, projetos de leis, etc. mencionados, não importanto tanto que o texto esteja em ordem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77abab6",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pp_reader = pp.PdfReader(open('pg_0001.pdf', 'rb'))\n",
    "\n",
    "texto = pp_reader.pages[0].extract_text()\n",
    "\n",
    "texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5efb81",
   "metadata": {},
   "source": [
    "### Capturando CPFs\n",
    "\n",
    "Vamos procurar por todos os CPFs mencionados nesta página:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2257ad",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "re.findall(r'\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}', texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e095013a",
   "metadata": {},
   "source": [
    "E procurar por todas as datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8d113",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "re.findall(r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1b900",
   "metadata": {},
   "source": [
    "**Pergunta**: Qual a utilidade do `\\b` no regex acima?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ec1139d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "O `\\b` vem do inglês \"Word Boundary\" que significa \"limite da palavra\", ou seja, é esperado que naquela posição do padrão se esteja no limite da palavra.\n",
    "Neste caso está servindo de garantia que se está procurando por uma data sem contaminação. Por exemplo \"125812/09/200801\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-desert",
   "metadata": {},
   "source": [
    "# Exercícios\n",
    "\n",
    "Considere a base de notícias recém extraída para os exercícios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-designer",
   "metadata": {},
   "source": [
    "**Exercício 1)** Utilize RegEx para criar uma nova coluna no DataFrame. Esta nova coluna deve indicar se o título da notícia tem faz ou não menção ao **governo**.\n",
    "\n",
    "Aqui, você vai ter que pensar em um RegEx que busque por termos que façam sentido e generalizem a noção de **\"governo\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e0ee1e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pattern_government = r\"\\bpresidente|govern(?:ador|adora|o)|prefeut(?:o|a|ura)|minist(?:r[oa]s?|érios?)|embaixad(?:a|or|ora)|parlament(?:o|ar)|aliad(?:[oa]s?)|brasil|acordo|bc\\b\"\n",
    "matches = df[\"Titulo\"].str.contains(pattern_government, flags=re.IGNORECASE)\n",
    "\n",
    "df[\"governo_titulo\"] = matches.astype(\"category\").cat.rename_categories({\n",
    "    True: \"sim\",\n",
    "    False: \"não\",\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-facial",
   "metadata": {},
   "source": [
    "**Exercício 2)** Repita o exercício anterior, buscando por termos que façam menção à **bolsas de valores**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94cde3b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pattern_stock_exchanges = r\"\\bbolsas?|ibovespa|sp500|nasdaq|alta|baixa|acionistas?|fundo|crescimento\\b\"\n",
    "matches = df[\"Titulo\"].str.contains(pattern_stock_exchanges, re.IGNORECASE)\n",
    "\n",
    "df[\"bolsas_titulo\"] = matches.astype(\"category\").cat.rename_categories({\n",
    "    True: \"sim\",\n",
    "    False: \"não\",\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-invasion",
   "metadata": {},
   "source": [
    "**Exercício 3)** Repita o exercício anterior, buscando por termos que façam menção à **bolsas de valores** ou ao **governo** na **descrição** das notícias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b237b25",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "matches_government = df[\"Descrição\"].str.contains(pattern_government, re.IGNORECASE)\n",
    "matches_stock_exchanges = df[\"Descrição\"].str.contains(pattern_stock_exchanges, re.IGNORECASE)\n",
    "matches = matches_government | matches_stock_exchanges\n",
    "\n",
    "df[\"governo_bolsas_descricao\"] = matches.astype(\"category\").cat.rename_categories({\n",
    "    True: \"sim\",\n",
    "    False: \"não\",\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-female",
   "metadata": {},
   "source": [
    "**Exercício 4)** Utilizando as variáveis novas criadas com RegEx:\n",
    "\n",
    "\n",
    "**a)** Conte quantas notícias fazem menção ao Governo. Conte para o título e também descrição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9cfffc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "count_governo = df[\"governo_titulo\"].value_counts() + \\\n",
    "                df[\"governo_bolsas_descricao\"].value_counts()\n",
    "\n",
    "f\"{count_governo['sim']} notícias fazem menção ao Governo.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-sodium",
   "metadata": {},
   "source": [
    "**b)** Gere um gráfico de barras das frequências absolutas (contagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1474768",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[:, 4:7].value_counts([\"governo_titulo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-parcel",
   "metadata": {},
   "source": [
    "**c)** E se quisermos a frequência relativa (porcentagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-college",
   "metadata": {},
   "source": [
    "**Exercício 5)** Calcule a frequência de menções à Pandemia por seção (política, economia, finanças). Obs: Considere a descrição da notícia. Gere um gráfico de barras dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5138b3f6",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pattern_pandemia = r\"\\bpandemia|covid|coronavirus\\b\"\n",
    "\n",
    "df.loc[:, (\"Descrição\", \"Secao\")].groupby(\"Secao\").sum()[\"Descrição\"].str.count(pattern_pandemia, re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ea123",
   "metadata": {},
   "source": [
    "**Exercício 6)** Crie um código python que consiga fazer o download de várias páginas do diário oficial.\n",
    "\n",
    "Salve em arquivos nomeados no padrão `'pg_0001.pdf'`, `'pg_0002.pdf'`, ... , `'pg_000n.pdf'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a96c5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "num_paginas_diario_oficial = 5\n",
    "\n",
    "for index in range(1, num_paginas_diario_oficial+1):\n",
    "    filename = f\"pg_{index:04}.pdf\"\n",
    "    pdf_url=f\"http://diariooficial.imprensaoficial.com.br/doflash/prototipo/2023/Abril/19/exec1/pdf/\" + filename\n",
    "    response = urllib.request.urlopen(pdf_url)\n",
    "\n",
    "    with open(f'notebooks/lessons/09/diario_oficial/{filename}', 'wb') as pdf_file:\n",
    "        pdf_file.write(response.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34f0da",
   "metadata": {},
   "source": [
    "**Exercício 7)** Crie um código python que leia `n` arquivos **PDF**s de uma pasta e extraia seus textos utilizando a biblioteca vista no exemplo da aula.\n",
    "\n",
    "Aqui, cada **PDF** é uma página do diário oficial. Suponha que os arquivos estão nomeados no padrão `'pg_0001.pdf'`, `'pg_0002.pdf'`, ... , `'pg_000n.pdf'`.\n",
    "\n",
    "Retorne uma lista onde cada item da lista é uma string contendo o texto da página em questão. Exemplo:\n",
    "\n",
    "```python\n",
    "[\n",
    "    'texto da página 01 do diário`,\n",
    "    'texto da página 02 do diário`,\n",
    "    'texto da página 03 do diário`,\n",
    "    'texto da página 04 do diário`,\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40895624",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "directory = os.listdir(\"notebooks/lessons/09/diario_oficial/\")\n",
    "pdf_filenames = [filename for filename in directory if filename.endswith(\".pdf\")]\n",
    "\n",
    "pdf_texts = []\n",
    "\n",
    "for filename in pdf_filenames:\n",
    "    file_path = f\"notebooks/lessons/09/diario_oficial/{filename}\"\n",
    "    with open(file_path, 'rb') as pdf_file:\n",
    "        pdf = pp.PdfReader(pdf_file)\n",
    "\n",
    "        pdf_texts.append(pdf.pages[0].extract_text() )\n",
    "\n",
    "pdf_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168194b0",
   "metadata": {},
   "source": [
    "**Exercício 8)** Crie um código python que recebe a lista do exercício anterior. Exemplo:\n",
    "\n",
    "```python\n",
    "[\n",
    "    'texto da página 01 do diário`,\n",
    "    'texto da página 02 do diário`,\n",
    "    'texto da página 03 do diário`,\n",
    "    'texto da página 04 do diário`,\n",
    "]\n",
    "```\n",
    "\n",
    "Você deve procurar todos os **CPF**s ou **CNPJ**s contidos no diário. Indique a página onde o mesmo foi encontrado. Exemplo de resposta:\n",
    "```python\n",
    "[\n",
    "    ['123.456.789-00', 0],\n",
    "    ['87.340.538/0001-23', 0]\n",
    "    ['555.666.777-00', 1],\n",
    "    ['30.375.316/0001-29', 3],\n",
    "    ['30.375.316/0001-29', 3],\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef87eb9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "cpfs = [re.findall(r'\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}', text) for text in pdf_texts]\n",
    "cpfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a210b5",
   "metadata": {},
   "source": [
    "**Exercício 9)** Um empresário deseja saber quais páginas do diário oficial fazem menção a determinado assunto.\n",
    "\n",
    "Faça um programa em python que recebe os textos das páginas do diário oficial:\n",
    "\n",
    "\n",
    "```python\n",
    "[\n",
    "    'texto da página 01 do diário`,\n",
    "    'texto da página 02 do diário`,\n",
    "    'texto da página 03 do diário`,\n",
    "    'texto da página 04 do diário`,\n",
    "]\n",
    "```\n",
    "\n",
    "Crie um padrão de expressão regular para busca por termos/palavras (simule um cenário) e retorne uma lista das páginas que fazem menção ao padrão. Exemplo de resposta:\n",
    "\n",
    "\n",
    "```python\n",
    "[0, 2, 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbbffdb",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pattern = r\"\\bsão paulo\\b\"\n",
    "re_pattern = re.compile(pattern, re.IGNORECASE)\n",
    "\n",
    "matches = [re_pattern.findall(text) for text in pdf_texts]\n",
    "pages = [index for index, page_matches in enumerate(matches) if len(page_matches) > 0]\n",
    "\n",
    "pages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
